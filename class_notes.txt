

Container Lifecycle

    docker container run = 'docker container create' + 'docker container start'

Project
  Node JS Web App

Advanced Web App
    Node.js and Redis


Microservices - Advantages
  - Language independent
  - Fast iterations
  - Smaller teams
  - Fault isolation
  - Pairs well with containers
  - Dynamically Scalable

Microservices - Disadvantages
  - Complex networking
  - Overhead
      Databases
      Servers


Docker
  Containers are a way to package software in a format that can run isolated on a
  shared OS. Unlike VMs, containers do not bundle a full OS - only libraries and
  settings required to make the software work are needed. This makes for efficient,
  lightweight, self-contained systems and guarantees that software will always run
  the same, regardless of where it is deployed."

  Completely isolated environments.

  Share same O.S. kernel of host system.

  Docker is a based LXC container.

  Building Blocks of Containers:

    Namespaces
      User
      PID
      Mount
      Network
      IPC
      UTC (Unix Time Sharing)

    cgroups
      Ability to limit resources CPU, memory, network, etc..
      Resource limits on system resources CPU, memory, network, etc..
      Prioritization
      Accounting
      Control

    copy-on-write storage

  Docker Images

    A read-only template with instructions on how to create a container.

  Container

    A runnable instance of a docker image.

  Init Container

    Any pod spec can have containers and InitContainers

    Same level as a container.

    Init Containers are run in the order listed (one by one) in the YAML file and before
    the real workload i.e. containers run.

    Each Init Container MUST complete (exit) successfully, if any Init Container fails
    the pod fails.

    After all init containers have run successfully normal containers are started





  Dockerfile

    Describes the build process for an image.

    Can be run to automatically create an image.

    Contains all the commands necessary to build the image and run the application.


Container Orchestration
  Deploying and scaling containers

Container Runtime Environment
  Docker
  containerd
  RKT
  CRI-O


Kubernetes
  An open source system for automating deployment, scaling and management of
  containerized applications.

  APIs are declaritive not imperative.

  Kubernetes objects are persistent entities in the Kubernetes system.
  Kubernetes uses these entities to represent the state of your cluster.
    What containerized applications are running and on which nodes.
    The resources available to those applications.
    The policies around how those applications behave, such as restart
    policies, upgrades, and fault-tolerance.

  A kubernetes object is a record of intent once the object is created kubernetes
  will constantly work to ensure that object exists.

  Control Plane
    A set of services that control the kubernetes cluster.
    Makes global decisions about the cluster and detect and respond to
    cluster events (e.g. starting up a new pod whe a replication controller's
    replicas field is unsatisfied.)

  kube-apiserver
    Primary management component for kubernetes.
    Entrypoint to K8S cluster
    UI/API/CLI access via api-server
    runs on port 6443
    responsible for orchestration of all operations within the cluster

    kube-apiserver is the only process/service that interacts with the ETCD cluster

    Steps taken by the API-server to create a pod:
    1) Authenticate User by the api-server
    2) Validate Request by the api-server
    3) Retrieve data from etcd
    4) Update ETCD (only kube-apiserver interacts with etcd)
    5) Scheduler identifies the correct worker node to place the pod on
       The scheduler communicates back to the api-server which then updates
       etcd.
    6) Kubelet running on the node specified by the scheduler is notified by
       the api-server, then the kubelet creates the pod on the node and instructs
       container runtime engine to deploy the application image.
       The kublet then notifies the api-server that the pod has been created, then
       the api-server updates the information in etcd.



  etcd (separate piece of software not part of kubernetes.io)
    ETCD is a distributed reliable key-value store that is Simple,
    Secure and Fast.

    Holds current state of K8S cluster.

    CRITICAL.

    What is a key value store?
      Stores data in key:value pairs.
      Can't have duplicate keys.
      Strongly consistent
      Used to store and retrieve small amts of data such as configuration data
      that requires fast read and writes.
      Can't be used as a replacement for a tabular/relation DB.
      Stores information about:
        nodes
        pods
        configs
        secrets
        accounts
        roles
        bindings
        OTHERS

    # TO CONNECT TO ETCD POD:
    kubectl exec --stdin --tty etcd-k8s-master -n kube-system  -- /bin/sh

    To list ETCD keys:
    kubectl exec --stdin --tty etcd-controller-01 -n kube-system  -- etcdctl get / --prefix --keys-only

    runs on ports 2379, 2380 and 2381
    stores information about the kubernetes cluster
    etcd is a distributed reliable key-value store that is simple, secure
    and fast.

    To write data use:
      etcdctl set key1 value1

    To retrieve data use:
      etcdctl get key1

    RAFT protocol

    etcd directory structure

      Registry (root directory of etcd)

        minions
        pods
        ReplicaSets
        deployments
        roles
        secrets

    To find etcd configuration:
    kubeadm install:

      kubectl describe pods --namespace=kube-system etcd-controller-01 (this name will be different
      depending on the cluster setup)

        cat /etc/kubernetes/manifests/etcd.yaml

    Basic etcdctl commands:

    List all keys in etcd (note that myetcdctl is an alias that I use)
      myetcdctl get / --prefix --keys-only

    List users:
      myetcdctl get / users

    For HA ETCD in the /etc/kubernetes/mainfest/etcd.yaml file
      --initial-cluster controller-0=https://${CONTROLLER0_IP}:2380,controller-1=https://${CONTROLLER1_IP}:2380
      --initial-cluster-state new

    Basic ETCD Commands
      etcdctl snapshot save
      etcdctl endpoint health
      etcdctl get
      etcdctl put

    Full command including cert paths for etcdctl

    ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt \
                          --key=/etc/kubernetes/pki/etcd/server.key member list

  kube-controller-manager

    Keeps overview of what's happening in the cluster. i.e. container restart
    runs on port 10257
    checks to see if desired state == actual state

    continuiously monitors the state of pods and the nodes w/in the cluster
    through the apiserver

    Watch status
    Remediate Situation

    Node Controller
      Node Monitor Period is 5 seconds
      Node Monitor Grace Period is 40 seconds
      POD eviction timeout is 5 minutes

    Replication Controller (older depricated need to use replicaSet instead)
      Monitors the status of replicatSets ensuring that desired number of pods
      are available at all times within the set.
      If a pods dies the replication controller will create another pod.
      Provides high availability for applications.
      Also used for load balancing and scaling.

    Many other types of controllers such as:
      Deployment Controller
      Namespace Controller
      Endpoint Controller
      Stateful-Set
      PV-Protection Controller
      Service Account Controller
      Cronjob
      Job-controller
      Service-Account Controller
      PV-Binder-Controller
      PV-Protection-Controller
      ....

      All of these controllers listed above are part of the kube-controller-manager
      process.
      By default all controllers are enabled by default. You can configure which
      controllers are enabled.

    cloud-controller-manager
      Handles interaction with underlying cloud providers.

  kube-scheduler

    Ensures pod placement on nodes in cluster based on node resources i.e.
     CPU/Memory, etc.. runs on port 10259

    Responsible for scheduling applications/containers on worker nodes.

    Is not responsible for starting the pods only placement of pods on
    which nodes.

    The scheduler goes through two phases in determining pod placement.
      Filter Nodes:
      1. Filter nodes out that don't fit physical requirements i.e.
         memory/cpu for the pods.

      Rank Nodes:
      2. Rank nodes from 0 to 10 after placing the pod on the node.

      Also consider resource requirements and limits.


    Taints and Tolerations

      Sets restrictions on which pods can be scheduled to run on a node.
      Tells node to only accept pods with a certain toleration
      Taints are set on nodes in a cluster.
      Tolerations are set on pods.

      To taint a node:

        kubectl taint nodes "node-name" key=value:taint-effect

        For example:

        kubectl taint nodes node1 app=blue:NoSchedule

      There are three taint effects:

        NoSchedule - Pods will not be scheduled on the node
        PreferNoSchedule - The scheduler will try to not schedule pods on the node
        NoExecute - New pods will be not be scheduled on the node and any pods running
                    on the node that don't tolerate the taint will be evicted.

      By default no pod has any set tolerations.

      To remove the taint from the master/controlplane node:

        kubectl taint node k8s-master node-role.kubernetes.io/master:NoSchedule-  <-- The - is very IMPORTANT

      To add a toleration for a specific taint:

        kubectl


    Node Selectors/Affinity

      Selectors:
        To restrict a pod to run on a particular node.
        Can't use advaced expressions like OR or NOT:
        example: Large OR Medium?
                 NOT Small

      To ensure pods are hosted on particular nodes.

      Affinity:
        Can use operators like:
          In
          NotIn
          Exists
          DoesNotExist
          Gt
          Lt

      Node Affinity Types:

        Type 1: preferredDuringSchedulingIgnoredDuringExecution
        Type 2: requiredDuringSchedulingIgnoredDuringExecution
        Type 3: requiredDuringSchedulingRequiredDuringExecution

                  DuringScheduling    DuringExecution
        Type 1    Required              Ignored
        Type 2    Preferred             Ignored
        Type 3    Required              Required


    Taints and Tolerations vs Node Affinity


    Resource Requirements and Limits

      Default resource limits for the entire cluster can be set in the LimitRange
      namespace. Specifically mem-limit-range and cpu-limit-range


      Resource Requests
        Are the minimum amount of cpu/memory/disk needed for each container in the pod
        to run

      Resource Limits
        Are the maximum amount of cpu/memory/disk allowed for each container in the pod
        to run

      The LimitRange K8S object can be created to limit the compute resources within
      a given namespace.

      CPU:
        Minimum value is 1m or 0.1 == 100m (milli)
        1 CPU is one vCPU
        Default for K8S is 1 vCPU per container
        A pod can NOT use more CPU resources than it's limit.

      Memory:
        256Mi
        A pod CAN use more memory resources than it's limit.

        1G (Gigabyte) = 1,000,000,000 bytes
        1M (Megabyte) = 1,000,000 bytes
        1K (Kilobite) = 1,000 bytes

        1Gi (Gibibyte) = 1,073,741,824 bytes  2^30
        1Mi (Mebibytes) = 1,048,576 bytes or 2^20
        1Ki (kibibytes) = 1,024 bytes 2^10

      spec:
        containers:
        - name: simple-web
          image: simple-web
          ports:
            - containerPort: 8080
          resources:
            requests:
              memory: "1Gi"
              cpu: 4
            limits:
              memory: "2Gi"
              cpu: 8


    Manual Scheduling
      Must set nodeName to a value in the YAML definition file at creation time,
      Or the pod will be in a PENDING state.


    Multiple Schedulers

      IMPORTANT when creating a custom scheduler the option --leader-elect MUST be
      set to false.
      IMPORTANT also set a custom name for the scheduler with --scheduler-name=custom-scheduler

      When creating a custom scheduler in a multi-master setup the --lock-object-name must
      also be set.

      For a pod to use the custom scheduler in the pod definition file you need to set
      schedulerName to the name of the custom scheduler under containers dictionary.



    kubelet
      Runs on each worker node. Port 10250
      The 'captain' on the ship.
      Listens for instructions from the api-server on port 10250
      Registers a worker node with a kubernetes cluster
      Requests the container runtime engine to pull the required image
      Creates/Manages containers/pods.
      Monitors nodes and Pods.
      Configuration files are located under:
        /etc/kubernetes
        /var/lib/kubelet

      cAdvisor a sub-component of kubelet responsible for collecting performance
      metrics from pods.

      To enable Metrics Server:
      git clone https://github.com/kubernetes-incubator/metrix-serve
      Then kubectl create -f deploy/1.8+/

      To view performance: kubectl top node, kubectl top pod

    kube-proxy
      Looks for new services and creates rules using IPtables to forward traffic
      to services..
      Manages communication for services between nodes within the cluster.
      Manages communication between pods
      Uses IPtables rules to forward traffic to correct pod
      Pod network is a flat virtual network. All pods and services can communicate
      with each other. Exceptions when you've implemented network policies that
      restrict network access.

  Network overlay (calico, etc.)

    Default encapsulation is IP-in-IP.
    Uses BGP (Border Gateway Protocol) to share and exchange routing information b/w
    nodes to facilitate POD network communcation.


  coredns (DNS)


  Other addon services:
  WebUI aka dashboard
  cluster level logging
  container resource monitoring


  Pods Running On The Control Plan

    etcd: the etcd server
    kube-apiserver: the API server
    kube-controller-manager and kube-scheduler are the control plan components
    coredns: provides DNS-based service discovery
    kube-proxy: is the per-node component managing port mappings, etc..
    <net name> : is optional (per node) component managing the network overlay
    the READY column indicates the number of containers in each pod
    Note: this only shows containers you won't see host services
    Also Note: you may see different namespaces depending on setup


  CNI (Container Network Interface)
    Most Kubernetes clusters use CNI "plugins" to implement networking
    When a pod is created, Kubernetes delegates the network setup to these plugins
    (it can be a single plugin, or a combination of plugins, each doing one task)

    Typically, CNI Plugins will:
      allocate an IP address (by calling an IPAM plugin)
      add a network interface into the pods network namespace
      configure the interface as well as required routes, etc.

    The "pod-to-pod network" or "pod network":
      provides communication b/w pods and nodes
      is generally implemented with CNI plugins like Cilium, Flannel and Calico

    The "pod-to-service-network":
      provides internal communication and load balancing
      is generally implemented with kube-proxy (or maybe kube-router)

    Network Policies:
      Provide firewall and isolation
      can be bundled with the "pod network" or provided by another component

    Inbound traffic can be handled by multiple components
      something like kube-proxy or kube-router (for NodePort services)
      load balancers (ideally, connected to the pod network)

    It is possible to use multiple pod networks in parallel
      with "meta-plugins" like CNI-Genie or Multus

    Some solutions can fill multiple roles
      e.g. kube-router can be setup to provide the pod network and/or network
      policies and/or replace kube-proxy

  CSI (Container Storage Interface)

  CRI (Container Runtime Interface)



Kubernetes Components

  Node
    virtual or physical machine
    Kublet (runs on port 10250)
    kube-proxy (runs on port 10256)
    container runtime (docker, cri-o, containerd)
    Communicates with the master
    Runs pods

  Pods (is a definition)
    Smallest deployable unit of computing that can be created and managed by
    kubernetes.

    Usually one container per pod, but not always the case.

    A pod is a single instance of an application.

    An abstraction over a container.

    Each pod has it's own IP address.

    A pod is a group of one or more containers with shared storage/network
    resources and specification for how to run the containers.

    Exists on a node

    Pod IPs are ephemeral (temporary)

    Pods have level 3 (IP) connectivity, but services are level 4 (TCP or UDP)
    ports

    /pause container is ALWAYS present w/in a pod.
    The purpose of the /pause container:
      Makes sure there is a network stack to map and does not change.
      /pause container holds the cgroups, reservations and namespaces of a pod
      before the container is created
      aka helper container.
      Share the same network and storage space as the container in the pod.

    Resource Requests:
      Allow you to define an amount of resources (such as CPU and/or Memory) you
      expect the container to use.
      The kubernetes scheduler will use resource requests to avoid scheduling pods
      on nodes that do not have enough available resources.

    Resource Limits:
      Provide a way for you to limit the amount of resources your container(s) can
      use. The container runtime is responsible for enforcing these limits and
      different container runtimes do this differently.

      To set resource limits you must set the kind LimitRange in the name space
      for example:
        apiVersion: v1
        kind: LimitRange
        metadata:
          name: mem-limit-range
        spec:
          limits:
          - default:
              memory: 512Mi
            defaultRequest:
              memory: 256Mi
            type: Container

        https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/


        apiVersion: v1
        kind: LimitRange
        metadata:
          name: cpu-limit-range
        spec:
          limits:
          - default:
              cpu: 1
            defaultRequest:
              cpu: 0.5
            type: Container

        https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/


  Multi-container Pods

    Example pairing together a web server and log agent.
    These apps will share the same network space and application life cycle.


  Namespace:
    A feature/module of the Linux kernel that allow you to provide a "view" to
    the process that hides everything outside of the namespace, thus giving its
    own environment to run it. Basically it makes it so processes that
    run within the namespace can't see or interfere with other processes

    cgroups:
      Limits the resources a process can use.

    Namespaces include:
      Hostname
      Process IDs
      File System
      Network Interfaces
      Inter-Process Communication (IPC)
      UTS (UNIX Time Share)

      Simple way to create a process namespace:
        sudo unshare --fork --pid --mount-proc bash

  Kubernetes Namespaces
    Default namespaces created when a new kubernetes cluster is created

        kube-system
          Do NOT create or modify in kube-system
          System processes for cluster
          Master and kubectl processes

        kube-node-lease
          heartbeats of nodes
          each node has an associated lease object in namespace
          determines the availability of a node

        kube-public
          Publically accessible data
          A configmap, which contains cluster information (kubectl
          cluster-info)

        default
          resources you create are located here unless you create a custom
          namespace

    Why use a namespace?
      Isolation of resources between environments i.e. Prod, Dev, etc..
      Group resources into namespaces.
      Each namespace can have their own set of policies defining who can do what.
      Multiple teams using same application stack.
      Blue/Green deployments
      Limit resources

      Namespace DNS

          service Name
           |
           |      Namespace
           |      |    service
           |      |   |         domain
           |      |   |        |
      db-service.dev.svc.cluster.local

      Characteristics of namespaces?
        You can't access MOST resources from another namespace.
        Example, each namespace would need to define their own configMap even
        if using a shared resource like a database.
        Secrets.
        Services can be shared accross namespaces.
          Example:
            ...
            data:    # service name.NAMESPACE #
            db_url: mysql-service.database

        Components that can't be created w/in a namespace
          live globally in a cluster
          you can't isolate them

          Examples: Volumes, Nodes

          Lists out all API resources that are not bound by namespaces:

            kubectl api-resources --namespaced=false

          Lists out all API resources that are BOUND by namespaces:

            kubectl api-resources --namespaced=true

      Change active namespace to the development namespace permanently:

        kubectl config set-context $(kubectl config current-context) --namespace=development


  Static Pods:
    Are pods managed directly by the kubelet daemon on a specific node, without
    the API server observing them.

    The kubelet daemon periodically checks /etc/kubernetes/manifests for pod
    configuration files to create a pod.

    If the application crashes the kublet daemon will attempt to restart it.

    If you remove a configuration file for a pod, the kublet daemon will
    remove the pod.

    For kubeadmin installs the staticPodPath will be /etc/kubernetes/mainfests.
    Check /var/lib/kubelet/config.yaml for staticPodPath

    Can't be used to deploy K8S objects such as deployments, daemonsets or replicasets

    Why use static pods?
      When using the kubeadm tool it uses static pods to build out the control
      plane components.

    Static Pods vs DaemonSets

    Static Pods                                       DaemonSets
    Created by kubelet                                Created by kube-apiserver (DaemonSet controller)
    Deploy control plane components as static pods.   Deploy monitoring agents, logging agents on nodes
                                                      Ignored by the kube-scheduler


  Kubernetes Kind List:

    Kind                                                Version
    pod                                                 v1
    namespace                                           v1
    ReplicationController (manages pods)                v1
    StatefulSet                                         apps/v1
    DaemonSet                                           apps/v1
    Service                                             v1
    ConfigMap                                           v1
    Volume                                              v1
    NetworkPolicy
    StorageClass                                        storage.k8s.io/v1
    PersistentVolume                                    v1
    PersistentVolumeClaim                               v1


  Services

    Service cidr range (default) is 10.96.0.0/12

    kind: Service
    What is a service?
      Provide a way to expose an application running as a set of pods.

      Enable communication b/w various components within and outside of an
      application. Services help connection applications together with other
      applications and users.

      Services are kubernetes objects that reside in memory.

      Pods are ephemeral and are destroyed frequently.

      A service provides a stable/persistent IP address to access the pods.

      Also provides loadbalancing

      A group of endpoints

      ClusterIP (default service type and is also an internal service)
        Also, have multi-port ClusterIP service

      Service communication: selectors
        A selector is a key value pair that will which pods to forward traffic to.

        Which pods to forward the request to?
          the service uses selector(s)
        Pods are identified via selectors which are defined in the metadata portion
        of the YAML file.

        ---
        apiVersion: v1
        kind: Service
        metadata:
          name: myapp-service
        spec:
          type: NodePort
          # This object is an ARRAY and you can have multiple elements in the array
          ports:
            - targetPort: 80 # Port on the POD
              port: 80 # Port on the SERVICE
              nodePort: 30008 # Port on the NODE
          # The selector connects the SERVICE to the PODS running the application
          selector:
            app: myapp # Must match labels in pod definition spec
            type: front-end # Must match labels in pod definition spec


      Endpoints are the backend entities to which Services route traffic. For a
      service that routes traffic to mutilple pods, each pod will have an endpoint
      associated with the service.

    The Four Service Type Attributes
      ClusterIP (Default type)
        Default, type not needed
        Internal service
        Only accessible within the cluster.
        Use them when your clients will be other pods within the cluster.
        A virtual IP address is allocated for the service (in an internal, private
        range).
        This IP address is reachable only from w/in the cluster (nodes and pods).
        Our code can connect to the service using the original port number.

      NodePort
        Allows for external traffic that has access to a fixed port on each worker
        node port range is between 30000 - 32767, anything outside that range won't
        be accepted
        A port is allocated for the service (by default in the 30000 - 32768 range).
        That port is made available on all our nodes and anybody can connect to it.
        Our code must be changed to connect to the new port number.

        There are three ports involved with NodePort type service:
          TargetPort is the port running on the POD which the service forwards requests
          to. If no port number is specified for the TargetPort the Port number will
          be used.

          Port is the port running on the service. Specifying Port is REQUIRED.

          NodePort which is used to access the service externally. The port number
          is from 30000 to 32767. If no port number is specified one is automatically
          assigned from 30000 to 32767.


      Loadbalancer
        Service becomes available through a cloud provided load balancer think AWS
        ALB.
        NodePort and ClusterIP service are created automatically when using
        Loadbalancer service type. Is an extension of the NodePort service type and
        the NodePort service is an extension of the ClusterIP service.


      Headless/ExternalName Service Type
        When a client wants to communicate with 1 specific Pod directly
        Or when pods want to talk directly with a specific pod without going
        through a service

        What is a headless service used case?
          Stateful applications like databases (MongoDB, MySQL, Elasticsearch,
          CockroachDB)

          If we want to save a tiny bit of latency < 1ms
          If we need to connect over arbitrary ports (instead of a few fixed ports)
          If we need to communicate over another protocol than UDP or TCP
          If we want to decide how to balance the requests client-side

        The DNS entry managed by CoreDNS will just be a CNAME to a provided record.
        No port, no IP address, nothing else is allocated.

        Client needs to figure out IP addresses of pods
          Option 1 - API call to K8S API Service
            makes app too tied to K8S API
            inefficient
          Option 2 - DNS Lookup
            DNS Lookup for services returns a single IP address (ClusterIP)
            So, set ClusterIP to "None" in the YAML file and the lookup will return
            Pod IP address listed in the service instead


    Handles requests (internal to the cluster or external)
    Usually a load balancer
    Expose a group of pods
    Durable VIP (or not if one chooses)
    Port and protocol
    Used to build service discovery

    When to use a service:


  Ingress
    kind: Ingress
    What is ingress?
      Is a kubernetes object that manages external access to Services in a
      kubernetes cluster.

      An ingress is capable of providing more functionality than a simple NodePort
      service, such ass SSL termination, advanced load balancing or name-based
      virtual hosting.

      Ingress objects actually do nothing by themselves. In order for Ingresses to
      do anything, you must install one or more Ingress Controllers.

      Exposes HTTP and HTTPS routes from outside the cluster to services within the
      cluster. Traffic routing is controlled by rules defined on the ingress
      resource. L7 HTTP routing

      HTTP proxy/routing rules

      L7 layer routing.

      API to match hostnames and URL paths

      Targets a service for each rule.

      Kubernetes defines the API but implementations are 3rd party

      Integrates with cloud providers (AWS, GCP, Azure) load balancers


    Ingress Controller
      evaluates all the rules defined in your cluster
      manages redirections
      entrypoint to the cluster
      many third-party implementations such as K8s Nginx Ingress controller,
      Traefik, etc..


  ConfigMap
    kind: configMap
    External configuration of the/your application stored in the format of a
    key:value map.
    Example: database URL, database user, password (not recommended)

    Configuration Volumes:
      configuration data from ConfigMaps and secrets can also be passed to
      containers in the form of mounted volumes. This will cause the configuration
      data to appear in files available to the container file system.


  Secrets
    kind: Secret
    Similar to ConfigMap, but used to securely store data like user passwords, SSL
    certs, etc..
    base64 encoding

    To encode a secret:

      echo -n "mysql" | base64
      bXlzcWw=

      Place resulting encoded data into the secrets definition yaml file.


  Volumes
    Physical storage attached/mounted to pods.
    The storage could be local, remote(NFS), SAN, iSCSI, etc..

    volumes: in the Pod spec, these specify the storage volumes available to the
    Pod. They specify the volume type and other data that determines where and how
    the data is actually stored.

    volumeMounts: In the Container spec, these reference the volumes in the Pod
    spec and provide a mountPath (the location on the file system where the
    container process will access the volume data)

    You can use volumeMounts to mount the same volume to multiple containers within
    the same Pod. This is a powerful way to have multiple containers interact with
    one another. For example, you could create a secondary sidecar coontainer that
    processes or transforms output from another container.

    Common Volume Types:

      hostPath: Stores data in a specified directory on the kubernetes node.

      emptyDir: A NON persistent ephemeral volume that stores data in a dynamically
      created location on the node. The directory exists only as long as the Pod
      exists on the node. The directory and the data are deleted when the Pod is
      removed. This volume type is very useful for simply sharing data between
      containers in the SAME Pod.

      Generic ephemeral volumes

      CSI ephemeral volumes

      configMaps

      secrets


  Persistent Volumes
    Are kubernetes objects that allow you to treat storage as an abstract resource
    to be consumed by Pods, much like kubernetes treats compute resources such as
    memory and CPU.

    A PersistentVolume usues a set of attributes to describe the underlying storage
    resource (such as a disk, cloud storage location) which will be used to store data.

    Storage Classes
    Allow kubernetes administrators to specify the types of storage they offer on
    their platform.

    The allowVolumeExpansion property of a StorageClass determines whether or not
    the StorageClass supports the ability to resize volumes after they're created.
    If this property is not set to true, attempting to resize a volume that uses
    this StorageClass will result in an error.

    A PersistentVolume's persistentVolumeReclaimPolicy determines how the storage
    resources can be reused when the PersistentVolume's associated
    PersistentVolumeClaims are deleted.

      Retain: Keeps all data. This requires an administrator to manually clean up
      the data and prepare the Storage resource for reuse.

      Delete: Deletes the underlying storage resource automatically (only works
      for cloud storage resources)

      Recycle: automatically deletes all the data in the underlying storage
      resource, allowing the PersistentVolume to be reused.

    Persistent storage falls under three different categories

      Network
        NFS

      Block
        fiber channel
        iSCSI

      Cloud
        AWS EBS
        Azure Disk
        GCE Persistent Disk


  StatefulSet
    Meant specifically for stateful applications like databases.
    Databases are often hosted outside of the K8S cluster.

  Labels and Selectors

    What is a label?
      Standard method to group K8S objects together.
      When we create a deployment with kubectl create deployment rng, this
      deployment gets the LABEL app=rng
      The replica sets created by this deployment also get the LABEL app=rng
      The pods created by these replica sets also get the LABEL app=rng
      When we create the daemon set from the deployment, we re-use the same spec
      Therefore, the pods created by the daemon set get the same LABELS
      When we use kubectl run "stuff", the LABEL is run=stuff instead

      Where do labels come from?
        When we create a deployment with kubectl create deployment rng,
        this deployment gets the label app=rng

    What is a selector?

      Selectors help to filter K8S objects.
      Selectors are in the higher level abstractions like replicatSets, DaemonSets,
      services and deployments.

      Two types of selectors:
        Equality-based
        Set-based

      A label selector can be made of multiple requirements which are comma-separated.
      In the case of multiple requirements, all must be satisfied so the comma separator
      acts as a logical AND (&&) operator.

    What do labels do?

    What do selectors do?
      Are the "glue" that connect resources together.
      A selector is how a service finds pods it's to send connections to.

    To get a list of pods for a specific LABEL use:
      kubectl get pods -l app=rng

    To get a list of pods for a specific SELECTOR use:
      kubectl get pods --selector app=rng


  ReplicaSet
    Makes sure that a given number of identical pods are running across the cluster
    allows for scaling of application rarely used directly

    The "mission" of a replica set is: "Make sure that there is the right number
    of pods matching this spec" if a pod fails create a new pod up to the number of
    replicas defined in the replicatSet

    To scale a replicatSet you can do one of the following:
      Update "name_of_file.yml" with number of replicas wanted then:

      kubectl replace -f "name_of_file.yml"

      kubectl scale --replicas=6 -f "name_of_file.yml"

      kubectl scale --replicas=6 replicaset "name_of_replicaset"

      kubectl apply -f "name_of_file.yml" <-- after updating # of replicas in file.

      kubectl edit replicaset "name_of_replicatset"

    Labels and Selectors are KEY for ReplicaSets and allows a ReplicaSet to know
    which pods to monitor in the event of pod failure.
    The "selector" is a key requirement for ReplicaSets


  DaemonSet
    Another way to deploy an application.
    When you do not want two instances of an application running on the same node
    to ensure the load is spread out.
    A daemonset will start one pod per node and keep it that way. So, if you have a
    node go away the pod will not restart on a different node.
    The use of LABELS and SELECTORS can be used to filter which nodes run a particular
    application in a daemon set.
    The "mission" of a daemon set is: "Make sure that there is a pod matching this
    spec on each node."

    How does it work (before kubernetes v1.12):
      set node name property on the pod before it is created

    How it now works:
      Uses NodeAfinity and the default scheduler.

    For example kube-proxy is deployed via daemonset.


  Deployment
    A kubernetes object that defines a desired state for a ReplicaSet (a set of
    replica pods). The Deployment Controller seeks to maintain the desired state by
    creating, deleting, and replacing Pods with new configurations.

    Defines desired state - kubernetes handles the rest
    Template/blueprint for creating pods.
    An abstraction of pods
    Meant for stateless applications.

    Allows for scaling, rolling updates, rollbacks
    Multiple deployments can be used together to implement a canary deployment
    Delegates pods management to replica sets

    Use a Deployment for stateless services, like frontends, where scaling up and
    down the number of replicas and rolling out updates are more important than
    controlling exactly which host the Pod runs on. Use a DaemonSet when it is
    important that a copy of a Pod always run on all or certain hosts, and when it
    needs to start before other Pods.

    A database (MySQL, Postgresql, etc) pods can't be replicated by a deployment,
    because it has state i.e. data that persists across reboots/destruction of pods

                      Service (resource)
           ________________________________________________
          |                                                |
          |          Deployment(resource)                  |
          |  --------------------------------------------- |
          |  |       ReplicaSet (resource)               | |
          |  | _________________________________________ | |
          |  | |                                       | | |
          |  | |       Pod (resource)                  | | |
          |  | | _____________________________________ | | |
          |  | | |                                   | | | |
          |  | | |            Container              | | | |
          |  | | |    ________________________       | | | |
          |  | | |    |                       |      | | | |
          |  | | |    |   ping 1.1.1.1        |      | | | |
          |  | | |    |                       |      | | | |
          |  | | |    |                       |      | | | |


  Deployment Step By Step

    1. The kubectl run command is executed:
      kubectl run web --image=nginx --replicas=3

    2. Communcation with the API Server then creates/stores deployment object in ETCD.

    3. Receive acknowledgement that the deployment object was stored in ETCD.

    4. API server Communicates with controller manager, the controller manager
       Communicates with ETCD via the API-Server to create the ReplicaSet.

    5. The controller manager will poll ETCD to see a new object i.e. ReplicaSet

    6. Scheduler polls ETCD via API Server

    7. Kubelet polls API Server to see if pods need to be CRUD.


  Monitoring Container Health with Probes
    Health checks are key to providing built-in Lifecycle automation
    Health checks are probes that apply to containers (not pods)
    Kubernetes will take action on containers that fail health checks
    Health checks are handled by the kubelet process on each node.


    Container health has three (optional) probes:

    Liveness Probes
      Allow you to automatically determine whether or not a container application
      is in a healthy state

      Indicates if the container is dead or alive
      A dead container cannot come back to life
      If liveness probe fails, the container is killed
      (to make really sure that it's really dead, no zombies or undeads)

      What happens next depends on the pod's restartPolicy:
        Never: the container is not restarted
        OnFailure or Always: the container is restarted

      When to use a liveness probe:
        To indicate failures that can't be recovered
          deadlocks (causing all requests to timeout)
          internal corruption (causing all requests to error)
      Anything where our incident response would be to "just restart/reboot it"


    Readiness Probes
      Are used to determine when a container is ready to accept requests.
      (only needed if pods are used in a service)

      Used in multi-container apps.
      Container is not killed
      if the pod is a member of a service, it is temporarily removed
      it is re-added as soon as the readiness probe passes again

      When to use a readiness probe:
        To indicate failure due to an external cause
          database is down or unreachable
          mandatory auth or other backend service unavailable

        To indicate temporary failure or unavailability
          application can only service N parallel connections
          runtime is busy doing garbage collection or initial data load

    Startup Probes
      Similar functionality as a liveness probe, startup probes run at container
      startup and stop running once they succeed.

    Different probe handlers are available (HTTP, TCP, program execution)

      HTTP request
        any status code b/w 200 and 399 indicates success

      TCP connection
        the probe succeeds if the TCP port is open

      arbitrary exec
        a command is executed in the container
        exit status of zero indicates success

      Timing and Thresholds
        periodSeconds
        timeoutSeconds
        successThreshold
        failureThreshold
        initialDelaySeconds

  Building Self-Healing Pods with Restart Policies

    Restart Policies
      Kubernetes can automatically restart containers when the fial. Restart
      policies allow you to customize this behavior by defining when you want a
      pod's container to be automatically restarted

      Restart policies are an important component of self-healing applications,
      which are automatically repaired when a problem arises.




Kubernetes Cluster Monitoring

  Open Source Monitoring solutions:
    Metrics Server (formaly known as Heapster)
    Prometheus
    ELK
    ... others

  Propritary Monitoring Solutions
    Datadog
    dynaTrace
    ... others

  Kubelet runs on each worker node and kubelet contains a sub-component called cAdvisor
  that is responsible for retrieving performance metrics from pods and exposing them
  through kubelet API to make the metrics available.


Networking Model
  One big flat IP network
  All nodes must be able to reach each other w/o NAT
  All pods must be able to reach each other w/o NAT
  Pods and nodes must be able to reach each other w/o NAT
  Each pod is aware of it's IP address (no NAT)
  Pod IP addresses are assigned by the network implementation (e.g calico network
  overlay)

  The Good
    Everything can reach everything
    No address translation
    No port translation
    No new network protocol
    The network implementation can decide how to allocate addresses
    IP addresses don't have to be "portable" from a node to another
    The specification is simple enough to allow many various implementations

  The Less Good
    Everything can reach everything
      If you want security you need to add network policies
      The network implementation you use needs to support network policies
    There are dozens of implementations
    Pods have level 3 (IP) connectivity but services are level 4 (TCP or UDP)
    kube-proxy is on the data path when connecting to a pod or container and is
    not fast (relies on userland proxying or iptables).


YAML File Explained

  Each YAML configuration file has 4 requirements
  1) apiVersion

  2) kind
      A string
      the type of object you're trying to create e.g. Pod, Serivce, Deployment, etc.
      Pod
      ReplicaSet
      Deployment
      Serivce
      ...

  3) metadata
      Is a dictionary
      The only allowed values are
        name:
        labels:

        name is a string
        labels are a dictionary and you can have any key:value pair you see fit

      data about the object
      name, labels, etc..
      a dictionary

  4) spec (specification)
      Is a dictionary


  ###
  ###IMPORTANT EXAM TIP!!!
  ###
  TO create YAML template files for pods, deployments, services, replicatSets

    pod:
      kubectl run nginx --image=nginx --dry-run=client -o yaml > file_name.yml

    deployment:
      kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > file_name.yml

      # For some reason you can't redirect output of command below to file.
      kubectl create deployment --replicas=4 --image=nginx nginx2 --dry-run=client -o yaml

      # To create a deployment named nginx right away
      kubectl create deployment nginx --image=nginx


  status
      Reflects desired state of a component
      K8S will update the status of a component continuiously

      Where does K8S get the status data?
        comes from etcd where it holds current status of any K8S component


Secrets:
  - kind: "Secret"
  - metadata/name: A random name you'd choose for the secret
  - type: "Opaque" is the default value for arbitrary key-vaule pairs, can also use
    different types "SSL", etc..
  - data:  the actual contents in key-value pairs.


Service configuration file:
  - kind: "Service"
  - metadata/name: A random name you'd choose for the metadata name key-value pairs
  - selector: to connect to the Pod through label


Ways to create resources

  kubectl run
    easy way to get started

  kubectl create <resource>
    explicit, but lacks some features
    can't pass command-line arguments to deployments
    creates resources if they don't exist
    if a resource already exist, the kubectl create doesn't alter them.

  kubectl create -f foo.yml or

  kubectl apply -f foo.yml (declaritive)
    all features are available
    requires writing YAML file
    declaritive
    creates resources if they don't exist
    if a resource already exists, update them (to match definition provided
    by the YAML frile)
    stores the manifest as an annotation in the resource
    Remember: CRUD (Create, Replace, Update, Delete)

  Creating Multiple Resources using a Manifest

  Each manifest must contain these four parts:
    kind: ...
    apiVersion: ...
    metadata:
      name: ...
      ...
    spec:
      ...
    ---
    kind: ...
    apiVersion: ...
    metadata:
      name: ...
      ...
    spec:
      ...


  A kubernetes mainfest must have the following 4 attributes for each resource
  declared

  kind: ...
  apiVersion: ...
  metadata: ...
    name: ...
    ...
  spec:
    ...
  ---         <-- separates resources
  kind: ...
  apiVersion: ...
  metadata: ...
    name: ...
    ...
  spec:
    ...


Application Lifecycle Management

  App Rollout, Rolling Upgrades and Rollbacks

  Deploy/Create app Deployment:
    kubectl apply -f deployment/my-deployment.yml

  Get status of deployment:
    kubectl get deployments

  Update Deployment Use set image in deployment:

    kubectl set image deployment/my-deployment nginx=nginx:1.21.1 --record

    or

  Update deployment definition file with new container image and run:

    kubectl apply -f "name_of_deployment_file"

  Get status of deployment:
    kubectl rollout status deployment my-deployment

  Show history of deployment:
    kubectl rollout history deployment my-deployment

  To Rollback/Undo an application deployment:
    kubectl rollout undo deployment my-deployment


  Two strategies when doing application updates.
  Recreate
    Scales down the deployment to zero, then scales up the deployment using the new
    application image.

  RollingUpdate (The Default Deployment Strategy)
    Scales the deployment up and down accordingly while deploying the new application
    image.


Working with ConfigMaps or Secrets

  Used to pass configuration data using key value pairs.

  1. Create the configmap
  2. Inject the configmap into the pod

  There are two ways to create a configmap

    Imperative:
      kubectl create configmap

      kubectl create configmap "configmap_name" --from-literal=key=value

      example:
      kubectl create configmap app-config --from-literal=APP_COLOR=blue \
      --from-literal=APP_MODE=prod

      from a file:
      kubectl create configmap "configmap_name" --from-file="path_to_file"

      example:
      kubectl create configmap app-config --from-file=app_config.properties

    Declaritive
      kubectl apply -f "name_of_config_map"

      Configmap file:
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: app-config
        data:
          APP_COLOR: blue
          APP_MODE: prod

  To view configmaps

    kubectl get configmaps

    kubectl describe configmaps


Troubleshooting Kubernetes

  API Server:

  If the Kubernetes API server is down, you will not be able to use kubectl to
  interact with the cluster.
  You may get a message that looks something like:

    kube-master$: kubectl get nodes
      The connection to the server localhost:6443 was refused -
      did you specify the right host or port?

  Possible fixes:
  Make sure docker and kublet services are up and running on your control plane
  node(s)

  Checking Node Status

  kubectl get nodes
  kubectl describe node 'node_name'

  If a node is having problems, it may be because a service is down on that
  particular node.

  Check kublet and container runtime service is running with no errors.

    systemctl status kubelet|containerd
    systemctl start kubelet|containerd
    systemctl enable kubelet|containerd

    Check system log files for errors.

  Check System Pods

  kubectl get pods -n kube-system
  kubectl describe pod 'pod_name' -n kube-system

  Cluster and Node Logs

  Service Logs
    journalctl -u kubelet
    journalctl -u docker
    journalctl -u containerd

  Cluster Component Logs
    The kubernetes cluster components have log output redirected to /var/log.

    /var/log/kube-apiserver.log
    /var/log/kube-scheduler.log
    /var/log/kube-controller-manager.log

    Note that these log files may not appear for kubeadm clusters, since some/all
    components run inside containers. In that case you can access them with kubectl
    logs.


Network Troubleshooting of Kubernetes

  To troubleshoot or sniff network traffic between two PODs do the following.
    1. Get the IP addresses of the pods using:
        kubectl get pods -o wide

    2. SSH to worker node(s) hosting the pods.

    3. Using tshark or tcpdump gather network packets between both pods.
       Need to produce network traffic using curl or other utility.

    4. tshark command:
       root@worker-01:~# tshark -i cali1e0ad1c2d44 -Y "ip.src == 10.240.182.194 && ip.dst == 10.240.171.3"

  kube-proxy and DNS

    In addition to checking on your kubernetes networking plugin, it may be a good
    idea to look at kube-proxy and the kubernetes DNS if you're experiencing issues
    within the kubernetes cluster network.

    In a kubeadm cluster the kubernetes DNS and kube-proxy run as Pods in the
    kube-system namespace.

  netshoot

    Tip: you can run a container in the cluster that you can use to run commands to
    test and gather information about network functionality.

    A VERY Good container to use to troubleshoot networking issues is:

    nicolaka/netshoot image is a great tool for this. The image contains a variety
    of networking exploration and Troubleshooting tools.

    db-service.dev.svc.cluster.local --> service_name.namespace.service.domain


Troubleshooting Applications

  Check Pod Status

    kubectl get pods

    kubectl describe pod 'pod_name'

  Running Commands Inside Containers

    kubectl exec 'pod_name' -c 'container_name -- command_to_run

    NOTE: You cannot use kubectl exec to run any software that is NOT present
    within the container.

  Checking Container Logs

    Container Logging

    kubectl logs

      If Pod contains only ONE container:

        kubectl logs 'pod_name'

      If Pod contains more than one container:

        kubectl logs 'pod_name' -c 'container_name'

  Accessing Logs From the CLI

    kubectl logs command has limitations:
      it cannot stream logs from multiple pods at a time
      when showing logs from multiple pods, it mixes them all together

    There is a better way to look at container logs


YAML BASICS

  Data storage system.
  It's technically a superset of JSON.
  Spaces NOT tabs.

  key/value pairs
  arrays/lists
  dictionary/maps

  To express date/time expressed by ISO 8601 standard

  birthday: 1967-08-25 15:23:19

  Kubernetes files are called "manifests"
  Each file can contain one or more manifests
  Each manifest describes an API object (deployment, service, pod, etc.)
  Each manifest needs four parts (root key:values in the file)
    apiVersion:
    kind:
    metadata:
    spec:

  A simple pod manifest in YAML:
    apiVersion: v1
    kind: Pod
    metadata:
      name: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.17.3

  Generate YAML from kubectl create:

  kubectl create namespace awesome-app -o yaml --dry-run
  apiVersion: v1
  kind: Namespace
  metadata:
    name: awesome-app

  Advantages of YAML
    Using YAML (instead of kubectl run/create/etc..) allows you to be declaritive
    The YAML describes the desired state of our cluster and applications
    YAML can be stored, versioned, archived (e.g. git repositories)
    To change resources, change YAML files (instead of using kubectl, edit/scale
    /label/etc..)
    Changes can be reviewd before being applied
    This workflow is sometimes called "GitOps"


Rolling Updates
  Rolling updates can be done on:
    deployments, daemonsets, StatefulSets

  With rolling updates when a Deployment is updated it happens progressively.
  The Deployment controls multiple ReplicaSets
  Each ReplicaSet is a group of identical Pods
  (with the same image, arguments, parameters...)
  During the rolling update we have at least two ReplicaSets
    the "new" set (corresponding to the "target" version)
    and at least one "old" set
  We can have multiple "old" sets
  (if we start another update before the first one is completed)

  Two parameters determine the pace of the rollout:
    maxUnavailable
    maxSurge
  They can be specified in absolute number of pods or a percentage of the
  replicas count.
  At any given time...
    there will always be at least replicas-maxUnavailable pods availabile
    there will never be more than replicas+maxSurge pods in total
    there will therefore be up to maxUnavailable+maxSurge pods being updated
  We have the possibility to rolling back to previous version


Imperative vs Declaritive

  Imperative: Step by step instructions. In the case of K8S the kubectl command.

    Create K8S Objects:
      Create a pod:
      kubectl run --image=nginx nginx-pod

      Create a deployment:
      kubectl create deployment --image=nginx nginx-deployment

      Create a service for the nginx-deployment:
      kubectl expose deployment nginx-deployment --port=80 --targetPort=80 --type NodePort

      kubectl create -f nginx.yaml

    Updating K8S Objects:
      Manually editing a live deployment:
      kubectl edit deployment nginx-deployment

      Scale up number of pods for nginx-deployment:
      kubectl scale deployment nginx-deployment --replicas=4

      Rollout upgrade to pod image for the nginx-deployment:
      kubectl set image deployment nginx-deployment nginx=nginx:1.18


    Updating K8S Objects Part II:

      To open up K8S config in memory in order to perform edits/updates.
      kubectl edit deployment nginx

      Better way is to update the YAML file with necessary changes then
      replace the K8S objects:

      kubectl replace -f nginx.yaml
      kubectl replace --force -f nginx.yaml

      kubectl delete -f nginx.yaml


  Declaritive: End state.
    Running the kubectl apply to create/update/delete a K8S object.

    Create K8S Objects:
      kubectl apply -f nginx.yaml

    To create K8S objects from multiple config files:

      kubectl apply -f /path/to/config-files/

    Update K8S Objects:
      kubectl apply -f nginx.yaml


Application Commands and Arguments In Dockerfile Definition Files

  Dockerfile CMD entry Examples

  Shell format
  CMD command param1          CMD sleep 5

  JSON Format
  CMD ["command", "param1"]   CMD ["sleep", "5"]

  If you wish to override the default entry in a Dockerfile CMD field append
  the command to the end of the docker container run command.

    docker container run ubuntu-sleeper sleep 10

  The above command isn't pretty we want something like this.

    docker container run ubuntu-sleeper 10

  So, you need to update the Dockerfile and an entry called ENTRYPOINT and
  rebuild the image.

  To set a default value for the command if one wasn't specified.

  To override the ENTRYPOINT you can use the --entrypoint option when executing docker run

    docker container run --entrypoint sleep2.0 ubuntu-sleeper 10


Application Commands and Arguments In Pod Definition File

  The ENTRYPOINT field corresponds to the command field in a pod definition file
  The CMD field corresponds to the args field in a pod definition file


Environment Variables



KUBECTL COMMANDS

  List cluster nodes:

  kubectl get nodes
  kubectl get nodes -o wide
  kubectl get nodes -o yaml

  List all cluster containers:

  kubectl get all --all-namespaces -o wide
  kubectl get nodes -o json| jq ".items[] | {name:.metadata.name} + .status.capacity"

  View definition (introspection documentation) of a filed in a resource:

  kubectl explain node.spec
  kubectl explain --recursive node.spec

  List API resources:
  kubectl api-resources

  Namespaces:
  kubectl get namespaces

  kubectl delete
  kubectl label


KUBECTL APPLICATION DEPLOYMENT

  kubectl apply -f "definition_file"


TO CONNECT TO RUNNING POD

  1. List pods in default namespace:
      kubectl get pods
  2. Select pod name you want to connect to:
      NAME                                   READY   STATUS    RESTARTS   AGE
      my-nginx-deployment-7f845c788b-lz55l   1/1     Running   5          37d
      my-nginx-deployment-7f845c788b-pr8dv   1/1     Running   5          37d
      static-busybox                         1/1     Running   0          3m2s

  3. Use kubectl to connect to pod:

      kubectl exec static-busybox -ti -- /bin/sh



KUBECTL DESCRIBE SERVICE
(kubectl describe works for all resource types e.g. node, service, etc..)

  kubectl describe pod/alpine

Various ways to create resources:
  kubectl run ...
  kubectl create <resource>
  kubectl create -f foo.yaml or kubectl apply -f foo.yaml

List running pods on worker nodes:
  kubectl describe nodes <node_name>

  kubectl get pods -l app=<service_name|deployment_name> -o wide

List endpoints of a deployment or service in YAML:
  kubectl get endpoints <service_name|deployment_name> -o yaml

Create Service Step by Step:

  Create Deployment Using nginx:
    kubectl create deployment bignginx --image=nginx:latest

  Create Service Running on Port 8889
    kubectl expose deployment bignginx  --port=8889 --target-port=80 --name=nginx-8889 --type=NodePort

  List all taints:
    kubectl get nodes -o json | jq '.items[].spec.taints'

  List taints by node:
    kubectl get nodes -o json | jq ".items[]|{name:.metadata.name, taints:.spec.taints}"

  Remove all taints from master node:
    kubectl taint nodes --all node-role.kubernetes.io/master-

  List Rolling Update Percentage:
    kubectl get deployments.apps -o json |

     ".items[] | {name:.metadata.name} + .spec.strategy.rollingUpdate"

  Generate YAML Dry-Run
    kubectl get deployments.apps dashboard -o yaml


Create an NGINX Pod:
  kubectl run nginx --image=nginx

Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run):
  kubectl run nginx --image=nginx --dry-run=client -o yaml

Create a Deployment called nginx:
  kubectl create deployment --image=nginx nginx

Generate Deployment YAML file (-o yaml). Don't create it(--dry-run):
  kubectl create deployment --image=nginx nginx --dry-run=client -o yaml

Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) with 4 Replicas (--replicas=4):
  kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yml

  The above command will save output to a file, make necessary changes to the file
  then create the deployment.

  kubectl create -f nginx-deployment.yml OR kubectl apply -f nginx-deployment.yml

  OR with K8s version 1.19+ we can specify the --replicas option to create a deployment with 4 replicas:
  kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yml




Labels
  To add label:
    kubectl label node kube-1 environment=production

  To remove label:
    kubectl label node kube-1 environment-



ETCDCTL COMMANDS

To get top level list of all keys in a cluster:
  myetcdctl get / --prefix --keys-only

To get values of keys:
  myetcdctl get / --prefix


#######################################
# ETCD BACKUP BEGIN
#######################################
List Cluster Members:

sudo ETCDCTL_API=3 etcdctl --endpoints 192.168.50.10:2379   --cert=/etc/kubernetes/pki/etcd/server.crt   --key=/etc/kubernetes/pki/etcd/server.key   --cacert=/etc/kubernetes/pki/etcd/ca.crt   member list


Get Cluster Name:

ETCDCTL_API=3 etcdctl get cluster.name --endpoints=https://10.0.1.101:2379 \
--cacert=/home/cloud_user/etcd-certs/etcd-ca.pem \
--cert=/home/cloud_user/etcd-certs/etcd-server.crt \
--key=/home/cloud_user/etcd-certs/etcd-server.key cluster.name beebox


Backup ETCD Data:

ETCDCTL_API=3 etcdctl snapshot save /home/cloud_user/etcd_backup.db  \
--endpoints=https://10.0.1.101:2379 \
--cacert=/home/cloud_user/etcd-certs/etcd-ca.pem \
--cert=/home/cloud_user/etcd-certs/etcd-server.crt \
--key=/home/cloud_user/etcd-certs/etcd-server.key


Verify ETCD Snapshot:

ETCDCTL_API=3 etcdctl --write-out=table snapshot status
/path/to/backup/file/etcd_backup.db

+----------+----------+------------+------------+
|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
+----------+----------+------------+------------+
| f5a1f70b |   833063 |       1523 |     4.5 MB |
+----------+----------+------------+------------+

#######################################
# ETCD BACKUP END
#######################################


##########################################
Restore ETCD BEGIN
##########################################


Stop etcd service
systemctl stop etcd.service


Remove etcd data

sudo rm -rf /var/lib/etcd/


Restore etcd data

sudo ETCDCTL_API=3 etcdctl snapshot restore /home/cloud_user/etcd_backup.db \
> --initial-cluster etcd-restore=https://10.0.1.101:2380 \
> --initial-advertise-peer-urls https://10.0.1.101:2380 \
> --name etcd-restore \
> --data-dir /var/lib/etcd


Correct Ownership on /var/lib/etcd

sudo chown -R etcd:etcd /var/lib/etcd


Start etcd service

sudo systemctl start etcd.service

Verify cluster name

ETCDCTL_API=3 etcdctl get cluster.name --endpoints=https://10.0.1.101:2379 \
--cacert=/home/cloud_user/etcd-certs/etcd-ca.pem \
--cert=/home/cloud_user/etcd-certs/etcd-server.crt \
--key=/home/cloud_user/etcd-certs/etcd-server.key cluster.name beebox

##########################################
Restore ETCD END
##########################################




Certification Tips - Imperative Commands with Kubectl

While you would be working mostly the declarative way - using definition files,
imperative commands can help in getting one time tasks done quickly, as well as
generate a definition template easily. This would help save considerable amount
of time during your exams.

Before we begin, familiarize with the two options that can come in handy while
working with the below commands:

--dry-run: By default as soon as the command is run, the resource will be created.
If you simply want to test your command , use the --dry-run=client option. This
will not create the resource, instead, tell you whether the resource can be created
and if your command is right.

-o yaml: This will output the resource definition in YAML format on screen.

Use the above two in combination to generate a resource definition file quickly,
that you can then modify and create resources as required, instead of creating
the files from scratch.

POD
Create an NGINX Pod

kubectl run nginx --image=nginx

Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)

kubectl run nginx --image=nginx --dry-run=client -o yaml


Deployment
Create a deployment

kubectl create deployment --image=nginx nginx

Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)

kubectl create deployment --image=nginx nginx --dry-run=client -o yaml


Generate Deployment with 4 Replicas

kubectl create deployment nginx --image=nginx --replicas=4


You can also scale a deployment using the kubectl scale command.

kubectl scale deployment nginx --replicas=4

Another way to do this is to save the YAML definition to a file and modify

kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml

You can then update the YAML file with the replicas or any other field before creating the deployment.


Service
Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml

(This will automatically use the pod's labels as selectors)

Or

kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml
(This will not use the pods labels as selectors, instead it will assume selectors as
app=redis. You cannot pass in selectors as an option. So it does not work very well if
your pod has a different label set. So generate the file and modify the selectors
before creating the service)



Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port
30080 on the nodes:

kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml

(This will automatically use the pod's labels as selectors, but you cannot specify the node port.
You have to generate a definition file and then add the node port in manually before creating the
service with the pod.)

Or

kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml

(This will not use the pods labels as selectors)

Both the above commands have their own challenges. While one of it cannot accept a selector the
other cannot accept a node port. I would recommend going with the kubectl expose command. If you
need to specify a node port, generate a definition file using the same command and manually input
the nodeport before creating the service.