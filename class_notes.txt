

Container Lifecycle

    docker container run = 'docker container create' + 'docker container start'

Project
  Node JS Web App

Advanced Web App
    Node.js and Redis


Microservices - Advantages
  - Language independent
  - Fast iterations
  - Smaller teams
  - Fault isolation
  - Pairs well with containers
  - Dynamically Scalable

Microservices - Disadvantages
  - Complex networking
  - Overhead
      Databases
      Servers


Docker
  Containers are a way to package software in a format that can run isolated on a
  shared OS. Unlike VMs, containers do not bundle a full OS - only libraries and
  settings required to make the software work are needed. This makes for efficient,
  lightweight, self-contained systems and guarantees that software will always run
  the same, regardless of where it is deployed."

  Completely isolated environments.

  Share same O.S. kernel of host system.

  Docker is a based LXC container.

  Building Blocks of Containers:

    Namespaces
      User
      PID
      Mount
      Network
      IPC

    cgroups
      Ability to limit resources CPU, memory, network, etc..

    copy-on-write storage

  Docker Images


  Dockerfile
    Describes the build process for an image.

    Can be run to automatically create an image.

    Contains all the commands necessary to build the image and run the application.


Container Orchestration
  Deploying and scaling containers

Container Runtime Environment
  Docker
  containerd
  RKT
  CRI-O


Kubernetes
  An open source system for automating deployment, scaling and management of
  containerized applications.

  APIs are declaritive not imparitive

  Control Plane
    A set of services that control the kubernetes cluster.
    Makes global decisions about the cluster and detect and respond to
    cluster events (e.g. starting up a new pod whe a replication controller's
    replicas field is unsatisfied.)

    kube-apiserver
      Primary management component for kubernetes.
      Entrypoint to K8S cluster
        UI/API/CLI access via api-server
        runs on port 6443
        responsible for orchestration of all operations within the cluster

      kube-apiserver is the only process/service that interacts with the ETCD cluster

      Steps taken by the API-server to create a pod:
      1) Authenticate User
      2) Validate Request
      3) Retrieve data
      4) Update ETCD (only kube-apiserver interacts with etcd)
      5) Scheduler
      6) Kubelet


    etcd (separate piece of software not part of kubernetes.io)
      A distributed Key:value database. Current state of K8S cluster.

      CRITICAL.

        What is a key value store?
          Stores data in key:value pairs.
          Can't have duplicate keys.
          Strongly consistent
          Used to store and retrieve small amts of data such as configuration data
          that requires fast read and writes.
          Can't be used as a replacement for a tabular/relation DB.
          Stores information about:
            nodes
            pods
            configs
            secrets
            accounts
            roles
            bindings
            OTHERS

      # TO CONNECT TO ETCD POD:
      kubectl exec --stdin --tty etcd-k8s-master -n kube-system  -- /bin/sh

      runs on ports 2379, 2380 and 2381
      stores information about the kubernetes cluster
      etcd is a distributed reliable key-value store that is simple, secure
      and fast.

      To write data use:
        etcdctl set key1 value1

      To retrieve data use:
        etcdctl get key1

      RAFT protocol

      etcd directory structure

        Registry (root directory of etcd)

                    minions
                    pods
                    ReplicaSets
                    deployments
                    roles
                    secrets

      To find etcd configuration:
        kubeadm install:
          kubectl describe pods --namespace=kube-system etcd-controller-01 (this name will be different
          depending on the cluster setup)

          cat /etc/kubernetes/manifests/etcd.yaml

          ps -ef|grep kube-apiserver


    kube-scheduler

      Ensures pod placement on nodes in cluster based on node resources i.e.
      CPU/Memory, etc.. runs on port 10259

      Responsible for scheduling applications/containers on worker nodes.

      Is not responsible for starting the pods only placement of pods on
      which nodes.

      The scheduler goes through two phases in determining pod placement.
        1. Filter nodes out that don't fit physical requirements i.e.
           memory/cpu for the pods.
        2. Rank nodes from 0 to 10 after placing the pod on the node.

        Also consider resource requirements and limits.


      Taints and Tolerations

        Sets restrictions on which pods can be scheduled to run on a node.
        Taints are set on nodes in a cluster.
        Tolerations are set on pods.

        By default no pod has any set tolerations.

      To remove the taint from the master/controlplane node:
      kubectl taint node k8s-master node-role.kubernetes.io/master:NoSchedule-


      Node selectors/Affinity

        To restrict a pod to run on a particular node.



      Filter Nodes


      Rank Nodes


      Resource Requirements



      Manual Scheduling
        Must set nodeName to a value in the YAML definition file at creation time,
        Or the pod will be in a PENDING state.


    kube-controller-manager

      Keeps overview of what's happening in the cluster. i.e. container restart
      runs on port 10257
      checks to see if desired state == actual state

      continuiously monitors the state of pods and the nodes w/in the cluster
      through the apiserver

      Watch status
      Remediate Situation

      Node Controller
        Node Monitor Period is 5 seconds
        Node Monitor Grade Period is 40 seconds
        POD eviction timeout is 5 minutes

      Replication Controller
        Monitors the status of replicatSets ensuring that desired number of pods
        are available at all times within the set.


      Many other types of controllers such as:
        Deployment Controller
        Namespace Controller
        Endpoint Controller
        Stateful-Set
        PV-Protection Controller
        Service Account Controller
        Job-controller
        Service-Account Controller
        PV-Binder-Controller
        PV-Protection-Controller
        ....

        All of these controllers listed above are part of the kube-controller-manager
        process.
        By default all controllers are enabled by default. You can configure which
        controllers are enabled.

      cloud-controller-manager
        Handles interaction with underlying cloud providers.


    kube-proxy
      Looks for new services
      Manages communication for services between nodes within the cluster.
      Manages communication between pods
      Uses IPtables rules to forward traffic to correct pod


    kubelet
      Runs on each worker node.
      The 'captain' on the ship.
      Listens for instructions from the api-server
      Registers a worker node in a kubernetes cluster
      Creates/Manages containers/pods.
      Monitors nodes and Pods.

      cAdvisor a sub-component of kubelet responsible for collecting performance
      metrics from pods.

      To enable Metrics Server:
      git clone https://github.com/kubernetes-incubator/metrix-serve
      Then kubectl create -f deploy/1.8+/

      To view performance: kubectl top node, kubectl top pod


    Network overlay (calico, etc.)


    coredns (DNS)


    Other addon services:
    WebUI aka dashboard
    cluster level logging
    container resource monitoring


    Pods Running On The Control Plan

      etcd: the etcd server
      kube-apiserver: the API server
      kube-controller-manager and kube-scheduler are the control plan components
      coredns: provides DNS-based service discovery
      kube-proxy: is the per-node component managing port mappings, etc..
      <net name> : is optional (per node) component managing the network overlay
      the READY column indicates the number of containers in each pod
      Note: this only shows containers you won't see host services
      Also Note: you may see different namespaces depending on setup

  CNI (Container Network Interface)
    Most Kubernetes clusters use CNI "plugins" to implement networking
    When a pod is created, Kubernetes delegates the network setup to these plugins
    (it can be a single plugin, or a combination of plugins, each doing one task)

    Typically, CNI Plugins will:
      allocate an IP address (by calling an IPAM plugin)
      add a network interface into the pods network namespace
      configure the interface as well as required routes, etc.

    The "pod-to-pod network" or "pod network":
      provides communication b/w pods and nodes
      is generally implemented with CNI plugins

    The "pod-to-service-network":
      provides internal communication and load balancing
      is generally implemented with kube-proxy (or maybe kube-router)

    Network Policies:
      Provide firewall and isolation
      can be bundled with the "pod network" or provided by another component

    Inbound traffic can be handled by multiple components
      something like kube-proxy or kube-router (for NodePort services)
      load balancers (ideally, connected to the pod network)

    It is possible to use multiple pod networks in parallel
      with "meta-plugins" like CNI-Genie or Multus

    Some solutions can fill multiple roles
      e.g. kube-router can be setup to provide the pod network and/or network
      policies and/or replace kube-proxy

Kubernetes Components

  Node
    virtual or physical machine
    Kublet (runs on port 10250)
    kube-proxy (runs on port 10256)
    container runtime (docker, cri-o, containerd)
    Communicates with the master
    Runs pods

  Pods (is a definition)
    Smallest deployable unit of computing that can be created and managed by
    kubernetes. Usually one container per pod, but not always the case.

    An abstraction over a container.

    Smallest object created in a kubernetes cluster.

    Each pod has it's own IP address.

    A pod is a group of one or more containers with shared storage/network
    resources and specification for how to run the containers.

    Exists on a node

    Pod IPs are ephemeral (temporary)

    Pods have level 3 (IP) connectivity, but services are level 4 (TCP or UDP)
    ports

    /pause container is ALWAYS present w/in a pod.
    The purpose of the /pause container:
      Makes sure there is a network stack to map and does not change.
      /pause container holds the cgroups, reservations and namespaces of a pod
      before the container is created
      aka helper container.
      Share the same network and storage space as the container in the pod.

    Resource Requests:
      Allow you to define an amount of resources (such as CPU and/or Memory) you
      expect the container to use.
      The kubernetes scheduler will use resource requests to avoid scheduling pods
      on nodes that do not have enough available resources.

    Resource Limits:
      Provide a way for you to limit the amount of resources your container(s) can
      use. The container runtime
      is responsible for enforcing these limits and different container runtimes
      do this differently.

      To set resource limits you must set the kind LimitRange in the name space
      for example:
        apiVersion: v1
        kind: LimitRange
        metadata:
          name: mem-limit-range
        spec:
          limits:
          - default:
              memory: 512Mi
            defaultRequest:
              memory: 256Mi
            type: Container

        https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/


        apiVersion: v1
        kind: LimitRange
        metadata:
          name: cpu-limit-range
        spec:
          limits:
          - default:
              cpu: 1
            defaultRequest:
              cpu: 0.5
            type: Container

        https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/


  Namespace:
    A feature/module of the Linux kernel that allow you to provide a "view" to
    the process that hides everything outside of the namespace, thus giving its
    own environment to run it. Basically it makes it so processes that
    run within the namespace can't see or interfere with other processes

    Namespaces include:
      Hostname
      Process IDs
      File System
      Network Interfaces
      Inter-Process Communication (IPC)

      Simple way to create a process namespace:
        sudo unshare --fork --pid --mount-proc bash

      Default namespaces created when a new kubernetes cluster is created

        kube-system
          Do NOT create or modify in kube-system
          System processes for cluster
          Master and kubectl processes

        kube-node-lease
          heartbeats of nodes
          each node has an associated lease object in namespace
          determines the availability of a node

        kube-public
          Publically accessible data
          A configmap, which contains cluster information (kubectl
          cluster-info)

        default
          resources you create are located here unless you create a custom
          namespace

      Why use a namespace?
        Group resources into namespaces.
        Multiple teams using same application stack.
        Blue/Green deployments
        Limit resources

        Characteristics of namespaces?
          You can't access MOST resources from another namespace.
          Example, each namespace would need to define their own configMap even
          if using a shared resource like a database.
          Secrets.
          Services can be shared accross namespaces.
            Example:
              ...
              data:    # service name.NAMESPACE #
              db_url: mysql-service.database

          Components that can't be created w/in a namespace
            live globally in a cluster
            you can't isolate them

            Examples: Volumes, Nodes

            Lists out all API resources that are not bound by namespaces:

              kubectl api-resources --namespaced=false

            Lists out all API resources that are BOUND by namespaces:

              kubectl api-resources --namespaced=true

      Change active namespace
        Use kubens/kubectx
        Not installed by default in a kubernetes cluster.

      Creates a pod in the default namespace:
      kubectl create -f pod-definition.yml

      Creates a pod in the dev namespace:
      kubectl create -f pod-definition.yml --namespace=dev

      or add the namespace definition into the YAML file under the metadata section.

    cgroups:
      Limits the resources a process can use.

    Static Pods:
      Are pods managed directly by the kubelet daemon on a specific node, without
      the API server observing them.

      The kubelet daemon periodically checks /etc/kubernetes/manifests for pod
      configuration files to create a pod.

      If the application crashes the kublet daemon will attempt to restart it.

      If you remove a configuration file for a pod, the kublet daemon will
      remove the pod.

      For kubeadmin installs the staticPodPath will be /etc/kubernetes/mainfests.
      Check /var/lib/kubelet/config.yaml for staticPodPath

Kubernetes Kind List:

  Kind                                                Version
  pod                                                 v1
  namespace                                           v1
  ReplicationController (manages pods)                v1
  StatefulSet                                         apps/v1
  DaemonSet                                           apps/v1
  Service                                             v1
  ConfigMap                                           v1
  Volume                                              v1
  NetworkPolicy
  StorageClass                                        storage.k8s.io/v1
  PersistentVolume                                    v1
  PersistentVolumeClaim                               v1


  Services

    kind: Service
    What is a service?
      Provide a way to expose an application running as a set of pods.

      Pods are ephemeral and are destroyed frequently.

      A service provides a stable/persistent IP address to access the pods.

      Also provides loadbalancing

      A group of endpoints

      ClusterIP (default service type and is also an internal service)
        Also, have multi-port ClusterIP service

      Service communication: selectors
        A selector is a key value pair that will which pods to forward traffic to.

        Which pods to forward the request to?
          the service uses selector(s)
        Pods are identified via selectors which are defined in the metadata portion
        of the YAML file.

      Endpoints are the backend entities to which Services route traffic. For a
      service that routes traffic to mutilple pods, each pod will have an endpoint
      associated with the service.

    The Four Service Type Attributes
      ClusterIP (Default type)
        Default, type not needed
        Internal service
        Only accessible within the cluster.
        Use them when your clients will be other pods within the cluster.
        A virtual IP address is allocated for the service (in an internal, private
        range).
        This IP address is reachable only from w/in the cluster (nodes and pods).
        Our code can connect to the service using the original port number.

      NodePort
        Allows for external traffic that has access to a fixed port on each worker
        node port range is between 30000 - 32767, anything outside that range won't
        be accepted
        A port is allocated for the service (by default in the 30000 - 32768 range).
        That port is made available on all our nodes and anybody can connect to it.
        Our code must be changed to connect to the new port number.

      Loadbalancer
        Service becomes available through a cloud provided load balancer think AWS
        ALB.
        NodePort and ClusterIP service are created automatically when using
        Loadbalancer service type. Is an extension of the NodePort service type and
        the NodePort service is an extension of the ClusterIP service.


      Headless/ExternalName Service Type
        When a client wants to communicate with 1 specific Pod directly
        Or when pods want to talk directly with a specific pod without going
        through a service

        What is a headless service used case?
          Stateful applications like databases (MongoDB, MySQL, Elasticsearch,
          CockroachDB)

          If we want to save a tiny bit of latency < 1ms
          If we need to connect over arbitrary ports (instead of a few fixed ports)
          If we need to communicate over another protocol than UDP or TCP
          If we want to decide how to balance the requests client-side

        The DNS entry managed by CoreDNS will just be a CNAME to a provided record.
        No port, no IP address, nothing else is allocated.

        Client needs to figure out IP addresses of pods
          Option 1 - API call to K8S API Service
            makes app too tied to K8S API
            inefficient
          Option 2 - DNS Lookup
            DNS Lookup for services returns a single IP address (ClusterIP)
            So, set ClusterIP to "None" in the YAML file and the lookup will return
            Pod IP address listed in the service instead


    Handles requests (internal to the cluster or external)
    Usually a load balancer
    Expose a group of pods
    Durable VIP (or not if one chooses)
    Port and protocol
    Used to build service discovery

    When to use a service:

  Ingress
    kind: Ingress
    What is ingress?
      Is a kubernetes object that manages external access to Services in a
      kubernetes cluster.

      An ingress is capable of providing more functionality than a simple NodePort
      service, such ass SSL termination, advanced load balancing or name-based
      virtual hosting.

      Ingress objects actually do nothing by themselves. In order for Ingresses to
      do anything, you must install one or more Ingress Controllers.

      Exposes HTTP and HTTPS routes from outside the cluster to services within the
      cluster. Traffic routing is controlled by rules defined on the ingress
      resource. L7 HTTP routing


    Ingress Controller
      evaluates all the rules defined in your cluster
      manages redirections
      entrypoint to the cluster
      many third-party implementations such as K8s Nginx Ingress controller,
      Traefik, etc..


  ConfigMap
    kind: configMap
    External configuration of the/your application stored in the format of a
    key:value map.
    Example: database URL, database user, password (not recommended)

    Configuration Volumes:
      configuration data from ConfigMaps and secrets can also be passed to
      containers in the form of mounted volumes. This will cause the configuration
      data to appear in files available to the container file system.

  Secrets
    kind: Secret
    Similar to ConfigMap, but used to securely store data like user passwords, SSL
    certs, etc..
    base64 encoding

  Volumes
    Physical storage attached/mounted to pods.
    The storage could be local, remote(NFS), SAN, iSCSI, etc..

    volumes: in the Pod spec, these specify the storage volumes available to the
    Pod. They specify the volume type and other data that determines where and how
    the data is actually stored.

    volumeMounts: In the Container spec, these reference the volumes in the Pod
    spec and provide a mountPath (the location on the file system where the
    container process will access the volume data)

    You can use volumeMounts to mount the same volume to multiple containers within
    the same Pod. This is a powerful way to have multiple containers interact with
    one another. For example, you could create a secondary sidecar coontainer that
    processes or transforms output from another container.

    Common Volume Types:

      hostPath: Stores data in a specified directory on the kubernetes node.

      emptyDir: A NON persistent volume that stores data in a dynamically created
      location on the node. The directory exists only as long as the Pod exists on
      the node. The directory and the data are deleted when the Pod is removed.
      This volume type is very useful for simply sharing data between containers in
      the SAME Pod.


  Persistent Volumes
    Are kubernetes objects that allow you to treat storage as an abstract resource
    to be consumed by Pods, much like kubernetes treats compute resources such as
    memory and CPU.

    A PersistentVolume usues a set of attributes to describe the underlying storage
    resource (such as a disk, cloud storage location) which will be used to store data.

    Storage Classes
    Allow kubernetes administrators to specify the types of storage they offer on
    their platform.

    The allowVolumeExpansion property of a StorageClass determines whether or not
    the StorageClass supports the ability to resize volumes after they're created.
    If this property is not set to true, attempting to resize a volume that uses
    this StorageClass will result in an error.

    A PersistentVolume's persistentVolumeReclaimPolicy determines how the storage
    resources can be reused when the PersistentVolume's associated
    PersistentVolumeClaims are deleted.

      Retain: Keeps all data. This requires an administrator to manually clean up
      the data and prepare the Storage resource for reuse.

      Delete: Deletes the underlying storage resource automatically (only works
      for cloud storage resources)

      Recycle: automatically deletes all the data in the underlying storage
      resource, allowing the PersistentVolume to be reused.


  StatefulSet
    Meant specifically for stateful applications like databases.
    Databases are often hosted outside of the K8S cluster.

  ReplicaSet
    Makes sure that a given number of identical pods are running across the cluster
    allows for scaling of application rarely used directly

    The "mission" of a replica set is: "Make sure that there is the right number
    of pods matching this spec" if a pod fails create a new pod up to the number of
    replicas defined in the replicatSet

    To scale a replicatSet you can do one of the following:
      Update "name_of_file.yml" with number of replicas wanted then:

      kubectl replace -f "name_of_file.yml"

      kubectl scale --replicas=6 -f "name_of_file.yml"

      kubectl scale --replicas=6 replicaset "name_of_replicaset"

      kubectl apply -f "name_of_file.yml" <-- after updating # of replicas in file.

    Labels and Selectors are KEY for ReplicaSets and allows a ReplicaSet to know
    which pods to monitor in the event of pod failure.
    The "selector" is a key requirement for ReplicaSets



  DaemonSet
    Another way to deploy an application.
    When you do not want two instances of an application running on the same node
    to ensure the load is spread out.
    A daemonset will start one pod per node and keep it that way. So, if you have a
    node go away the pod will not restart on a different node.
    The use of LABELS and SELECTORS can be used to filter which nodes run a particular
    application in a daemon set.
    The "mission" of a daemon set is: "Make sure that there is a pod matching this
    spec on each node."

    How does it work (before kubernetes v1.12):
      set node name property on the pod before it is created

    How it now works:
      Uses NodeAfinity and the default scheduler.


    For example kube-proxy is deployed via daemonset.

  Deployment
    A kubernetes object that defines a desired state for a ReplicaSet (a set of
    replica pods). The Deployment Controller seeks to maintain the desired state by
    creating, deleting, and replacing Pods with new configurations.

    Defines desired state - kubernetes handles the rest
    Template/blueprint for creating pods.
    An abstraction of pods
    Meant for stateless applications.

    Allows for scaling, rolling updates, rollbacks
    Multiple deployments can be used together to implement a canary deployment
    Delegates pods management to replica sets

    Use a Deployment for stateless services, like frontends, where scaling up and
    down the number of replicas and rolling out updates are more important than
    controlling exactly which host the Pod runs on. Use a DaemonSet when it is
    important that a copy of a Pod always run on all or certain hosts, and when it
    needs to start before other Pods.

    A database (MySQL, Postgresql, etc) pods can't be replicated by a deployment,
    because it has state i.e. data that persists across reboots/destruction of pods

                      Service (resource)
           ________________________________________________
          |                                                |
          |          Deployment(resource)                  |
          |  --------------------------------------------- |
          |  |       ReplicaSet (resource)               | |
          |  | _________________________________________ | |
          |  | |                                       | | |
          |  | |       Pod (resource)                  | | |
          |  | | _____________________________________ | | |
          |  | | |                                   | | | |
          |  | | |            Container              | | | |
          |  | | |    ________________________       | | | |
          |  | | |    |                       |      | | | |
          |  | | |    |   ping 1.1.1.1        |      | | | |
          |  | | |    |                       |      | | | |
          |  | | |    |                       |      | | | |


  Deployment Step By Step

    1. The kubectl run command is executed:
      kubectl run web --image=nginx --replicas=3

    2. Communcation with the API Server then creates/stores deployment object in ETCD.

    3. Receive acknowledgement that the deployment object was stored in ETCD.

    4. API server Communicates with controller manager, the controller manager
       Communicates with ETCD via the API-Server to create the ReplicaSet.

    5. The controller manager will poll ETCD to see a new object i.e. ReplicaSet

    6. Scheduler polls ETCD via API Server

    7. Kubelet polls API Server to see if pods need to be CRUD.


  Monitoring Container Health with Probes
    Health checks are key to providing built-in Lifecycle automation
    Health checks are probes that apply to containers (not pods)
    Kubernetes will take action on containers that fail health checks
    Health checks are handled by the kubelet process on each node.


    Container health has three (optional) probes:

    Liveness Probes
      Allow you to automatically determine whether or not a container application
      is in a healthy state

      Indicates if the container is dead or alive
      A dead container cannot come back to life
      If liveness probe fails, the container is killed
      (to make really sure that it's really dead, no zombies or undeads)

      What happens next depends on the pod's restartPolicy:
        Never: the container is not restarted
        OnFailure or Always: the container is restarted

      When to use a liveness probe:
        To indicate failures that can't be recovered
          deadlocks (causing all requests to timeout)
          internal corruption (causing all requests to error)
      Anything where our incident response would be to "just restart/reboot it"



    Readiness Probes
      Are used to determine when a container is ready to accept requests.
      (only needed if pods are used in a service)

      Used in multi-container apps.
      Container is not killed
      if the pod is a member of a service, it is temporarily removed
      it is re-added as soon as the readiness probe passes again

      When to use a readiness probe:
        To indicate failure due to an external cause
          database is down or unreachable
          mandatory auth or other backend service unavailable

        To indicate temporary failure or unavailability
          application can only service N parallel connections
          runtime is busy doing garbage collection or initial data load

    Startup Probes
      Similar functionality as a liveness probe, startup probes run at container
      startup and stop running once they succeed.

    Different probe handlers are available (HTTP, TCP, program execution)

      HTTP request
        any status code b/w 200 and 399 indicates success

      TCP connection
        the probe succeeds if the TCP port is open

      arbitrary exec
        a command is executed in the container
        exit status of zero indicates success

      Timing and Thresholds
        periodSeconds
        timeoutSeconds
        successThreshold
        failureThreshold
        initialDelaySeconds

  Building Self-Healing Pods with Restart Policies

    Restart Policies
      Kubernetes can automatically restart containers when the fial. Restart
      policies allow you to customize this behavior by defining when you want a
      pod's container to be automatically restarted

      Restart policies are an important component of self-healing applications,
      which are automatically repaired when a problem arises.

Networking Model
  One big flat IP network
  All nodes must be able to reach each other w/o NAT
  All pods must be able to reach each other w/o NAT
  Pods and nodes must be able to reach each other w/o NAT
  Each pod is aware of it's IP address (no NAT)
  Pod IP addresses are assigned by the network implementation (e.g calico network
  overlay)

  The Good
    Everything can reach everything
    No address translation
    No port translation
    No new network protocol
    The network implementation can decide how to allocate addresses
    IP addresses don't have to be "portable" from a node to another
    The specification is simple enough to allow many various implementations

  The Less Good
    Everything can reach everything
      If you want security you need to add network policies
      The network implementation you use needs to support network policies
    There are dozens of implementations
    Pods have level 3 (IP) connectivity but services are level 4 (TCP or UDP)
    kube-proxy is on the data path when connecting to a pod or container and is
    not fast (relies on userland proxying or iptables).



YAML File Explained

  Each YAML configuration file has 4 requirements
  1) apiVersion

  2) kind
      A string
      the type of object you're trying to create e.g. Pod, Serivce, Deployment, etc.
      Pod
      ReplicaSet
      Deployment
      Serivce
      ...

  3) metadata
      Is a dictionary
      The only allowed values are
        name:
        labels:

        name is a string
        labels are a dictionary and you can have any key:value pair you see fit

      data about the object
      name, labels, etc..
      a dictionary

  4) spec (specification)
      Is a dictionary




  TO create YAML template files for pods, deployments, services, replicatSets

    pod:
      kubectl run nginx --image=nginx --dry-run=client -o yaml > file_name.yml

    deployment:
      kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > file_name.yml

      # For some reason you can't redirect output of command below to file.
      kubectl create deployment --replicas=4 --image=nginx nginx2 --dry-run=client -o yaml


  status
      Reflects desired state of a component
      K8S will update the status of a component continuiously

      Where does K8S get the status data?
        comes from etcd where it holds current status of any K8S component


Secrets:
  - kind: "Secret"
  - metadata/name: A random name you'd choose for the secret
  - type: "Opaque" is the default value for arbitrary key-vaule pairs, can also use
    different types "SSL", etc..
  - data:  the actual contents in key-value pairs.

Service configuration file:
  - kind: "Service"
  - metadata/name: A random name you'd choose for the metadata name key-value pairs
  - selector: to connect to the Pod through label


#####
# Changes to kubectl run
#####

Before 1.18 use
kubectl run nginx --image nginx   created a Deployment named nginx (which creates
the ReplicaSet, which creates a Pod)

1.18+ use
kubectl run nginx --image nginx           creates a Pod named nginx
kubectl create deployment nginx --image   creates a Deployment named nginx

WARNING: One limit in 1.18 is there is no way to kubectl create deployment and also
run a custom command in an image.
You can create pods with custom commands using the new kubectl run, but if you want
to override the Dockerfile CMD in a deployment, you'll need to use a YAML manifest
to do it. This may affect a few examples in this course until all videos are
replaced with "1.18 compatible examples".


Ways to create resources

  kubectl run
    easy way to get started

  kubectl create <resource>
    explicit, but lacks some features
    can't pass command-line arguments to deployments
    creates resources if they don't exist
    if a resource already exist, the kubectl create doesn't alter them.

  kubectl create -f foo.yml or

  kubectl apply -f foo.yml (declaritive)
    all features are available
    requires writing YAML file
    declaritive
    creates resources if they don't exist
    if a resource already exists, update them (to match definition provided
    by the YAML frile)
    stores the manifest as an annotation in the resource
    Remember: CRUD (Create, Replace, Update, Delete)

  Creating Multiple Resources using a Manifest

  Each manifest must contain these four parts:
    kind: ...
    apiVersion: ...
    metadata:
      name: ...
      ...
    spec:
      ...
    ---
    kind: ...
    apiVersion: ...
    metadata:
      name: ...
      ...
    spec:
      ...



#######################################
# ETCD BACKUP BEGIN
#######################################
List Cluster Members:

sudo ETCDCTL_API=3 etcdctl --endpoints 192.168.50.10:2379   --cert=/etc/kubernetes/pki/etcd/server.crt   --key=/etc/kubernetes/pki/etcd/server.key   --cacert=/etc/kubernetes/pki/etcd/ca.crt   member list


Get Cluster Name:

ETCDCTL_API=3 etcdctl get cluster.name --endpoints=https://10.0.1.101:2379 \
--cacert=/home/cloud_user/etcd-certs/etcd-ca.pem \
--cert=/home/cloud_user/etcd-certs/etcd-server.crt \
--key=/home/cloud_user/etcd-certs/etcd-server.key cluster.name beebox


Backup ETCD Data:

ETCDCTL_API=3 etcdctl snapshot save /home/cloud_user/etcd_backup.db  \
--endpoints=https://10.0.1.101:2379 \
--cacert=/home/cloud_user/etcd-certs/etcd-ca.pem \
--cert=/home/cloud_user/etcd-certs/etcd-server.crt \
--key=/home/cloud_user/etcd-certs/etcd-server.key


Verify ETCD Snapshot:

ETCDCTL_API=3 etcdctl --write-out=table snapshot status
/path/to/backup/file/etcd_backup.db

+----------+----------+------------+------------+
|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
+----------+----------+------------+------------+
| f5a1f70b |   833063 |       1523 |     4.5 MB |
+----------+----------+------------+------------+

#######################################
# ETCD BACKUP END
#######################################


##########################################
Restore ETCD BEGIN
##########################################


Stop etcd service
systemctl stop etcd.service


Remove etcd data

sudo rm -rf /var/lib/etcd/


Restore etcd data

sudo ETCDCTL_API=3 etcdctl snapshot restore /home/cloud_user/etcd_backup.db \
> --initial-cluster etcd-restore=https://10.0.1.101:2380 \
> --initial-advertise-peer-urls https://10.0.1.101:2380 \
> --name etcd-restore \
> --data-dir /var/lib/etcd


Correct Ownership on /var/lib/etcd

sudo chown -R etcd:etcd /var/lib/etcd


Start etcd service

sudo systemctl start etcd.service

Verify cluster name

ETCDCTL_API=3 etcdctl get cluster.name --endpoints=https://10.0.1.101:2379 \
--cacert=/home/cloud_user/etcd-certs/etcd-ca.pem \
--cert=/home/cloud_user/etcd-certs/etcd-server.crt \
--key=/home/cloud_user/etcd-certs/etcd-server.key cluster.name beebox

##########################################
Restore ETCD END
##########################################



Kubernetes Objects are persistent entities. Kubernetes uses these objects to
represent the state of the cluster.
Kubernetes objects specifically describe:
  What containerized applications are running(and on which nodes)
  The resources availabile to those applications
  The policies around how those applications behave, such as restart policies,
  upgrades and fault-tolerance



A kubernetes mainfest must have the following 4 attributes for each resource
declared

kind: ...
apiVersion: ...
metadata: ...
  name: ...
  ...
spec:
  ...
---         <-- separates resources
kind: ...
apiVersion: ...
metadata: ...
  name: ...
  ...
spec:
  ...

Labels and Selectors

  What is a label?
    When we create a deployment with kubectl create deployment rng, this
    deployment gets the LABEL app=rng
    The replica sets created by this deployment also get the LABEL app=rng
    The pods created by these replica sets also get the LABEL app=rng
    When we create the daemon set from the deployment, we re-use the same spec
    Therefore, the pods created by the daemon set get the same LABELS
    When we use kubectl run "stuff", the LABEL is run=stuff instead

    Where do labels come from?
      When we create a deployment with kubectl create deployment rng,
      this deployment gets the label app=rng


  What is a selector?

    Selectors are in the higher level abstractions like replicatSets, DaemonSets,
    services and deployments.

    Two types of selectors:
      Equality-based
      Set-based

    A label selector can be made of multiple requirements which are comma-separated.
    In the case of multiple requirements, all must be satisfied so the comma separator
    acts as a logical AND (&&) operator.


  What do labels do?

  What do selectors do?
    Are the "glue" that connect resources together.
    A selector is how a service finds pods it's to send connections to.



  To get a list of pods for a specific LABEL use:
    kubectl get pods -l app=rng

  To get a list of pods for a specific SELECTOR use:
    kubectl get pods --selector app=rng



App Rollout, Rolling Upgrades and Rollbacks

Deploy app:
  kubectl apply -f deployment/my-deployment.yml

Get status of deployment:
  kubectl get deployments

Update NGINX Image in deployment:
  kubectl set image deployment/my-deployment nginx=nginx:1.21.1 --record

Get status of deployment:
  kubectl rollout status deployment my-deployment

Show history of deployment:
  kubectl rollout history deployment my-deployment

To Rollback/Undo an application deployment:
  kubectl rollout undo deployment my-deployment


Two strategies when doing application updates.
Recreate
  Scales down the deployment to zero, then scales up the deployment using the new
  application image.

RollingUpdate
  Scales the deployment up and down accordingly while deploying the new application
  image.


Troubleshooting Kubernetes

API Server:

  If the Kubernetes API server is down, you will not be able to use kubectl to
  interact with the cluster.
  You may get a message that looks something like:

    kube-master$: kubectl get nodes
      The connection to the server localhost:6443 was refused -
      did you specify the right host or port?

  Possible fixes:
  Make sure docker and kublet services are up and running on your control plane
  node(s)

Checking Node Status

  kubectl get nodes
  kubectl describe node 'node_name'

  If a node is having problems, it may be because a service is down on that
  particular node.
  Check kublet and container runtime service is running with no errors.

    systemctl status kubelet|containerd
    systemctl start kubelet|containerd
    systemctl enable kubelet|containerd

    Check system log files for errors.

Check System Pods

  kubectl get pods -n kube-system
  kubectl describe pod 'pod_name' -n kube-system

Cluster and Node Logs

  Service Logs
    journalctl -u kubelet
    journalctl -u docker
    journalctl -u containerd

  Cluster Component Logs
    The kubernetes cluster components have log output redirected to /var/log.

    /var/log/kube-apiserver.log
    /var/log/kube-scheduler.log
    /var/log/kube-controller-manager.log

    Note that these log files may not appear for kubeadm clusters, since some/all
    components run inside containers. In that case you can access them with kubectl
    logs.


Troubleshooting Applications

  Check Pod Status

    kubectl get pods

    kubectl describe pod 'pod_name'

  Running Commands Inside Containers

    kubectl exec 'pod_name' -c 'container_name -- command_to_run

    NOTE: You cannot use kubectl exec to run any software that is NOT present
    within the container.

  Checking Container Logs

    Container Logging

    kubectl logs

      If Pod contains only ONE container:

        kubectl logs 'pod_name'

      If Pod contains more than one container:

        kubectl logs 'pod_name' -c 'container_name'

  Accessing Logs From the CLI

    kubectl logs command has limitations:
      it cannot stream logs from multiple pods at a time
      when showing logs from multiple pods, it mixes them all together

    There is a better way to look at container logs



Troubleshooting Kubernetes Networking Issues

  kube-proxy and DNS

    In addition to checking on your kubernetes networking plugin, it may be a good
    idea to look at kube-proxy and the kubernetes DNS if you're experiencing issues
    within the kubernetes cluster network.

    In a kubeadm cluster the kubernetes DNS and kube-proxy run as Pods in the
    kube-system namespace.

  netshoot

    Tip: you can run a container in the cluster that you can use to run commands to
    test and gather information about network functionality.

    A VERY Good container to use to troubleshoot networking issues is:

    nicolaka/netshoot image is a great tool for this. The image contains a variety
    of networking exploration and Troubleshooting tools.

    db-service.dev.svc.cluster.local --> service_name.namespace.service.domain



YAML BASICS

  Data storage system.
  It's technically a superset of JSON.
  Spaces NOT tabs.

  key/value pairs
  arrays/lists
  dictionary/maps

  To express date/time expressed by ISO 8601 standard

  birthday: 1967-08-25 15:23:19

  Kubernetes files are called "manifests"
  Each file can contain one or more manifests
  Each manifest describes an API object (deployment, service, pod, etc.)
  Each manifest needs four parts (root key:values in the file)
    apiVersion:
    kind:
    metadata:
    spec:

  A simple pod manifest in YAML:
    apiVersion: v1
    kind: Pod
    metadata:
      name: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.17.3

  Generate YAML from kubectl create:

  kubectl create namespace awesome-app -o yaml --dry-run
  apiVersion: v1
  kind: Namespace
  metadata:
    name: awesome-app

Advantages of YAML
  Using YAML (instead of kubectl run/create/etc..) allows you to be declaritive
  The YAML describes the desired state of our cluster and applications
  YAML can be stored, versioned, archived (e.g. git repositories)
  To change resources, change YAML files (instead of using kubectl, edit/scale
  /label/etc..)
  Changes can be reviewd before being applied
  This workflow is sometimes called "GitOps"

Rolling Updates
  Rolling updates can be done on:
    deployments, daemonsets, StatefulSets

  With rolling updates when a Deployment is updated it happens progressively.
  The Deployment controls multiple ReplicaSets
  Each ReplicaSet is a group of identical Pods
  (with the same image, arguments, parameters...)
  During the rolling update we have at least two ReplicaSets
    the "new" set (corresponding to the "target" version)
    and at least one "old" set
  We can have multiple "old" sets
  (if we start another update before the first one is completed)

  Two parameters determine the pace of the rollout:
    maxUnavailable
    maxSurge
  They can be specified in absolute number of pods or a percentage of the
  replicas count.
  At any given time...
    there will always be at least replicas-maxUnavailable pods availabile
    there will never be more than replicas+maxSurge pods in total
    there will therefore be up to maxUnavailable+maxSurge pods being updated
  We have the possibility to rolling back to previous version


Imperative vs Declaritive

  Imperative: Step by step instructions
  Declaritive: End state.

Application Commands and Arguments In Pod Definition Files





KUBECTL COMMANDS

  List cluster nodes:

  kubectl get nodes
  kubectl get nodes -o wide
  kubectl get nodes -o yaml

  List all cluster containers:

  kubectl get all --all-namespaces -o wide
  kubectl get nodes -o json| jq ".items[] | {name:.metadata.name} + .status.capacity"

  View definition (introspection documentation) of a filed in a resource:

  kubectl explain node.spec
  kubectl explain --recursive node.spec

  List API resources:
  kubectl api-resources

  Namespaces:
  kubectl get namespaces

  kubectl delete
  kubectl label


KUBECTL APPLICATION DEPLOYMENT

  kubectl apply -f "definition_file"


TO CONNECT TO RUNNING POD

  1. List pods in default namespace:
      kubectl get pods
  2. Select pod name you want to connect to:
      NAME                                   READY   STATUS    RESTARTS   AGE
      my-nginx-deployment-7f845c788b-lz55l   1/1     Running   5          37d
      my-nginx-deployment-7f845c788b-pr8dv   1/1     Running   5          37d
      static-busybox                         1/1     Running   0          3m2s

  3. Use kubectl to connect to pod:

      kubectl exec static-busybox -ti -- /bin/sh



KUBECTL DESCRIBE SERVICE
(kubectl describe works for all resource types e.g. node, service, etc..)

  kubectl describe pod/alpine

Various ways to create resources:
  kubectl run ...
  kubectl create <resource>
  kubectl create -f foo.yaml or kubectl apply -f foo.yaml

List running pods on worker nodes:
  kubectl describe nodes <node_name>

  kubectl get pods -l app=<service_name|deployment_name> -o wide

List endpoints of a deployment or service in YAML:
  kubectl get endpoints <service_name|deployment_name> -o yaml

Create Service Step by Step:

  Create Deployment Using nginx:
    kubectl create deployment bignginx --image=nginx:latest

  Create Service Running on Port 8889
    kubectl expose deployment bignginx  --port=8889 --target-port=80 --name=nginx-8889 --type=NodePort

  List all taints:
    kubectl get nodes -o json | jq '.items[].spec.taints'

  List taints by node:
    kubectl get nodes -o json | jq ".items[]|{name:.metadata.name, taints:.spec.taints}"

  List Rolling Update Percentage:
    kubectl get deployments.apps -o json | jq ".items[] | {name:.metadata.name} + .spec.strategy.rollingUpdate"

  Generate YAML Dry-Run
    kubectl get deployments.apps dashboard -o yaml


Create an NGINX Pod:
  kubectl run nginx --image=nginx

Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run):
  kubectl run nginx --image=nginx --dry-run=client -o yaml

Create a Deployment called nginx:
  kubectl create deployment --image=nginx nginx

Generate Deployment YAML file (-o yaml). Don't create it(--dry-run):
  kubectl create deployment --image=nginx nginx --dry-run=client -o yaml

Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) with 4 Replicas (--replicas=4):
  kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yml

  The above command will save output to a file, make necessary changes to the file
  then create the deployment.

  kubectl create -f nginx-deployment.yml OR kubectl apply -f nginx-deployment.yml

  OR with K8s version 1.19+ we can specify the --replicas option to create a deployment with 4 replicas:
  kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yml




Labels
  To add label:
    kubectl label node kube-1 environment=production

  To remove label:
    kubectl label node kube-1 environment-



ETCDCTL COMMANDS

To get top level list of all keys in a cluster:
  myetcdctl get / --prefix --keys-only

To get values of keys:
  myetcdctl get / --prefix